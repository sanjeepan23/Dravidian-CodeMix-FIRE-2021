{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mBERT Uncased.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qxRJbroYV4I"
      },
      "source": [
        "###Installatioin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMhfj524YYNG"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install emoji\n",
        "!pip install emot\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-gpu\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE-xvHOxXDo2"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AK9MhhvXBZV"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from emot.emo_unicode import EMOTICONS\n",
        "import seaborn as sns\n",
        "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, LSTM\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "\n",
        "from nltk.corpus import words\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "import nltk, string, re, spacy,unicodedata, random\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "import nltk, string, re, spacy,unicodedata, random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNks3pI0W3Fe"
      },
      "source": [
        "#Mount google drive for retrive files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wImZH3xkQ7ji"
      },
      "source": [
        "###Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKQIjppiZUkf"
      },
      "source": [
        "#Load Training data\n",
        "train = pd.read_csv('/content/drive/My Drive/Sentiment Analysis Fire/data/tamil_sentiment_full_train.csv', names=['category','text'])\n",
        "train.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNVtjcSpZcBV"
      },
      "source": [
        "#Visualize Train\n",
        "train = train[['text', 'category']]\n",
        "sns.countplot(x='category', data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDQnm7iQZdsb"
      },
      "source": [
        "#Load Validation data\n",
        "val = pd.read_csv('/content/drive/My Drive/Sentiment Analysis Fire/data/tamil_sentiment_full_dev.csv', names=['category','text'])\n",
        "val.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNS6yapnZfMJ"
      },
      "source": [
        "#Visualize Val\n",
        "val = val[['text', 'category']]\n",
        "sns.countplot(x='category', data=val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bok8iZpFZgrx"
      },
      "source": [
        "#Load test data\n",
        "test = pd.read_csv('/content/drive/My Drive/Sentiment Analysis Fire/data/tamil_sentiment_full_test_withoutlabels.csv', names=['text'])\n",
        "test.head(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ6nRe0DoNti"
      },
      "source": [
        "# Select required columns\n",
        "train = train[['text', 'category']]\n",
        "val = val[['text', 'category']]\n",
        "\n",
        "# Remove a row if any of the two remaining columns are missing\n",
        "train = train.dropna();\n",
        "val = val.dropna()\n",
        "\n",
        "# Set your model output as categorical and save in new label col\n",
        "train['label_label'] = pd.Categorical(train['category'])\n",
        "val['label_label'] = pd.Categorical(val['category'])\n",
        "\n",
        "# Transform your output to numeric\n",
        "train['category'] = train['label_label'].cat.codes\n",
        "val['category'] = val['label_label'].cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjiZQskQRdFT"
      },
      "source": [
        "###Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-OjEDenRgU5"
      },
      "source": [
        "def convert_emoticons(text):\n",
        "  for emot in EMOTICONS:\n",
        "    text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "  text = emoji.demojize(text) #convert emojis to their defns in words, they might be useful\n",
        "  text = convert_emoticons(text)\n",
        "  text = re.sub(r'([\\.\\'\\\"\\/\\-\\_\\--])',' ', text) # remove punctuations , removes @USER / some abbreviatins\n",
        "  to_remove_url = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "      '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "  text = re.sub(to_remove_url,'',text)  # remove url patterns\n",
        "  text = re.sub(\" \\d+\", \" \", text)\n",
        "  text = text.replace(\",\",\" \")\n",
        "  text = re.sub(r'(?:^| )\\w(?:$| )', ' ', text).strip()\n",
        "  punctuation='!!\"$%&()*+-/:;<=>?[\\\\]^_{|}~.'\n",
        "  text = ''.join(ch for ch in text if ch not in set(punctuation))\n",
        "  # text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    # Stopword Removing\n",
        "  tokenizer = ToktokTokenizer()\n",
        "  # convert sentence into token of words\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  text = ' '.join(ch for ch in tokens)\n",
        "  return text \n",
        "\n",
        "def clean(df):\n",
        "  df['text'] = df['text'].apply(lambda x: preprocess(x))\n",
        "\n",
        "clean(train)\n",
        "clean(val)\n",
        "clean(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkvZJs5QQ-KG"
      },
      "source": [
        "#Setup BERT\n",
        "\n",
        "# Name of the BERT model to use\n",
        "model_name = 'bert-base-multilingual-uncased'\n",
        "\n",
        "# Load transformers config and set output_hidden_states to False\n",
        "config = BertConfig.from_pretrained(model_name)\n",
        "config.output_hidden_states = True\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
        "\n",
        "# Load the Transformers BERT model\n",
        "transformer_model = TFBertModel.from_pretrained(model_name, config = config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JklUuiM5Q_9D"
      },
      "source": [
        "# Load the MainLayer\n",
        "bert = transformer_model.layers[0]\n",
        "\n",
        "# Max length of tokens\n",
        "max_length = 100\n",
        "\n",
        "# Build your model input\n",
        "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
        "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "# Load the Transformers BERT model as a layer in a Keras model\n",
        "bert_model = bert(inputs)[1]\n",
        "dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
        "pooled_output = dropout(bert_model, training=True)\n",
        "\n",
        "# Then build your model output\n",
        "label = Dense(units=len(train.label_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='category')(pooled_output)\n",
        "relu_output = Dense(len(train.label_label.value_counts()), input_dim=len(train.label_label.value_counts()), activation='relu')(label)\n",
        "sigmoid_output = Dense(5, activation='sigmoid')(relu_output)\n",
        "outputs = {'category': sigmoid_output}\n",
        "\n",
        "# And combine it all in a model object\n",
        "model = Model(inputs=inputs, outputs=outputs, name='bert-base-multilingual-uncased')\n",
        "\n",
        "# Take a look at the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNfuPB_DRCWq"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "# Set an optimizer\n",
        "optimizer = Adam(\n",
        "    learning_rate=1e-05,\n",
        "    epsilon=1e-08,\n",
        "    decay=0.01,\n",
        "    clipnorm=1.0)\n",
        "\n",
        "# Set loss and metrics\n",
        "loss = {'category': CategoricalCrossentropy()}\n",
        "metric = {'category': CategoricalAccuracy('accuracy')}\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = loss, \n",
        "    metrics = metric)\n",
        "\n",
        "# Ready output data for the model\n",
        "y_label = to_categorical(train['category'])\n",
        "\n",
        "# Tokenize the input (takes some time)\n",
        "x = tokenizer(\n",
        "    text=train['text'].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    x={'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']},\n",
        "    y={'category': y_label},\n",
        "    validation_split=0.2,\n",
        "    batch_size=64,\n",
        "    epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3k4ExIcW5MZ"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/Sentiment Analysis Fire/models/preprocessed/mBERT-uncased.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz4e439BRC5E"
      },
      "source": [
        "#Evaluate the model\n",
        "\n",
        "# Ready test data`\n",
        "test_y_label = to_categorical(val['encode_cat'])\n",
        "test_x = tokenizer(\n",
        "    text=val['text'].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "\n",
        "# Run evaluation\n",
        "model_eval = model.evaluate(\n",
        "    x={'input_ids': test_x['input_ids'], 'attention_mask': test_x['attention_mask']},\n",
        "    y={'category': test_y_label}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZMxbvsHz-oh"
      },
      "source": [
        "#History object\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrT0cmkm0CBr"
      },
      "source": [
        "#plot the training and validation loss for comparison\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVXDyzTX0e4o"
      },
      "source": [
        "#plot the training and validation accuracy for comparison\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yslXgLYAIz2f"
      },
      "source": [
        "test_x = tokenizer(\n",
        "    text=test['text'].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "\n",
        "predictions = model.predict({'input_ids': test_x['input_ids'], 'attention_mask': test_x['attention_mask']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUfCMtCvSM_d"
      },
      "source": [
        "print(test['text'])\n",
        "print(len(predictions['category']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG5Tdl0UTDpA"
      },
      "source": [
        "###Write to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgo96zWlR7Vt"
      },
      "source": [
        "#Write out to the csv file\n",
        "arry = []\n",
        "\n",
        "data_test = np.array(test)\n",
        "label_array = np.array(predictions['category'])\n",
        "index = 1\n",
        "for i, j in zip(data_test, label_array):\n",
        "  text = i[0]\n",
        "  label_value_max = max(j)\n",
        "  label_index = np.where(label_array == label_value_max)\n",
        "\n",
        "  labels = ['unknown_state','Positive','Negative','Mixed_feelings','not-Tamil']\n",
        "  arry.append([index, text, labels[label_index[1][0]]])\n",
        "  #print(i[0] + ' - ' + labels[label_index[1][0]])\n",
        "  index = index + 1\n",
        "\n",
        "\n",
        "pre = pd.DataFrame(arry, columns=['text', 'category'])\n",
        "pre.to_csv('/content/drive/My Drive/Sentiment Analysis Fire/output/preprocessed/mBERT-uncased.csv', header=None, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}