{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DistilmBERT Cased.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFNSR0Idx2hx"
      },
      "source": [
        "###Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w0Lu5urD63k"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ArC2u6Ix7TD"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpi_lM3dklDn"
      },
      "source": [
        "#mporting the necessary libraries\n",
        "import emoji\n",
        "from emot.emo_unicode import EMOTICONS\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import re\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
        "from torch import cuda\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF7JlE3z4unw"
      },
      "source": [
        "#Mount google drive for retrive files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPev64RS1Si-"
      },
      "source": [
        "###Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc9yKKtbl572"
      },
      "source": [
        "#Load Training data\n",
        "train = pd.read_csv('/content/drive/My Drive/Sentiment Analysis Fire/preprocessing/train/transliterated_train.csv', names=['category','text'])\n",
        "train.category = train.category.apply({'unknown_state':0,'Negative':1,'not-Tamil':2,'Positive':3,'Mixed_feelings':4}.get)\n",
        "train.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFvSR2zaNiw5"
      },
      "source": [
        "print(encode_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwYXlCpqpYt5"
      },
      "source": [
        "#Visualize Train\n",
        "train = train[['text', 'category']]\n",
        "sns.countplot(x='category', data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xvGwVHc5Sev"
      },
      "source": [
        "#Load Validation data\n",
        "val = pd.read_csv('/content/drive/My Drive/Sentiment Analysis Fire/preprocessing/dev/transliterated_val.csv', names=['category','text'])\n",
        "val.category = val.category.apply({'unknown_state':0,'Negative':1,'not-Tamil':2,'Positive':3,'Mixed_feelings':4}.get)\n",
        "val.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK9VCgvcqk9J"
      },
      "source": [
        "#Visualize Val\n",
        "val = val[['text', 'category']]\n",
        "sns.countplot(x='category', data=val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaS4IqRO5ZLi"
      },
      "source": [
        "#Load test data\n",
        "test = pd.read_csv('/content/drive/My Drive/Sentiment Analysis Fire/preprocessing/test/transliterated_test.csv', names=['text'])\n",
        "test.head(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTHAKDWSv80F"
      },
      "source": [
        "print(test['text'][5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cj6pF7Gx0ez"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnBLWHNWx11a"
      },
      "source": [
        "#Initializing the key variables which will be later used in the training\n",
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-5\n",
        "distilbert_multilingual = 'distilbert-base-multilingual-cased'   #Pretrained model 01\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(distilbert_multilingual)   #load the model through tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3fD_4oIyiDG"
      },
      "source": [
        "##Prepare the dataset\n",
        "class SentimentDataset(Dataset):\n",
        "\n",
        "  def __init__(self,dataframe,tokenizer,max_len):\n",
        "    self.len = len(dataframe)\n",
        "    self.data = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len \n",
        "  \n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    sentence = str(self.data.text[index])\n",
        "    sentence = \" \".join(sentence.split())\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens = True,\n",
        "        max_length = self.max_len,\n",
        "        padding = 'max_length',\n",
        "        return_token_type_ids = False,\n",
        "        return_tensors = 'pt',\n",
        "        truncation = True\n",
        "    )\n",
        "    #ids = encoding['input_ids']\n",
        "    #mask = encoding['attention_mask']\n",
        "    return {\n",
        "        'ids' : encoding['input_ids'].flatten(),\n",
        "        'mask': encoding['attention_mask'].flatten(),\n",
        "        'targets': torch.tensor(self.data.category[index],dtype=torch.long)\n",
        "    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB_mASJm0tnH"
      },
      "source": [
        "#CXreate the dataloader for training\n",
        "print('Total no of entities in the dataset: {}'.format(train.shape))\n",
        "print('Train dataset:{}'.format(val.shape))\n",
        "\n",
        "training_set = SentimentDataset(train,tokenizer,MAX_LEN)\n",
        "validating_set = SentimentDataset(val,tokenizer,MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IAJRYDR1Wjb"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "val_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validating_loader = DataLoader(validating_set, **train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8EFGN972x96"
      },
      "source": [
        "# Fine-Tuning DistilBERT by adding a dropout and a dense layer on top of it to get the final output\n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")  #pretrained model\n",
        "        \n",
        "        self.lstm = nn.LSTM(768, 256, batch_first=True,bidirectional=True)\n",
        "        self.linear = nn.Linear(256*2, 5)\n",
        "        \n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 5) #Classifier layer with 5 class output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdL0nZCT5Wfa"
      },
      "source": [
        "model = DistillBERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4sI4pdN3EGv"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfqCZyyS5bp8"
      },
      "source": [
        "#Defining the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(params= model.parameters(),lr = LEARNING_RATE)\n",
        "#loss_function.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTjE1aDl57nx"
      },
      "source": [
        "def calcuate_accuracy(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs7HFEwZZV4Y"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AGaOSAd7zzG"
      },
      "source": [
        "# Defining the training function for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "  \n",
        "  tr_loss = 0\n",
        "  n_correct = 0\n",
        "  nb_tr_steps = 0\n",
        "  nb_tr_examples = 0\n",
        "  model.train()\n",
        "  start_time = time.time()\n",
        "  for _,data in enumerate(training_loader, 0):\n",
        "      ids = data['ids'].to(device, dtype = torch.long)\n",
        "      mask = data['mask'].to(device, dtype = torch.long)\n",
        "      targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(ids, mask)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      tr_loss += loss.item()\n",
        "      big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "      n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "      nb_tr_steps += 1\n",
        "      nb_tr_examples+=targets.size(0)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      #When using GPU\n",
        "      optimizer.step()\n",
        "\n",
        "  print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "  epoch_loss = tr_loss/nb_tr_steps\n",
        "  epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "  print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "  print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGXZaCxBLxkS"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  train(epoch)\n",
        "  print() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpuiWR_U-X_"
      },
      "source": [
        "#save the trained model\n",
        "torch.save(model.state_dict(), \"/content/drive/My Drive/Sentiment Analysis Fire/models/preprocessed+translation+transliterated/distilmBERT-cased.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6XP-E3f8yRw"
      },
      "source": [
        " def valid(model,testing_loader):\n",
        "  model.eval()\n",
        "  n_correct = 0\n",
        "  n_wrong = 0\n",
        "  total = 0\n",
        "  tr_loss = 0\n",
        "  nb_tr_steps = 0\n",
        "  nb_tr_examples = 0\n",
        "  with torch.no_grad():\n",
        "    for _,data in enumerate(testing_loader,0):\n",
        "      ids = data['ids'].to(device,dtype = torch.long)\n",
        "      mask = data['mask'].to(device,dtype = torch.long)\n",
        "      targets = data['targets'].to(device,dtype=torch.long)\n",
        "      outputs = model(ids,mask).squeeze()\n",
        "      loss = loss_function(outputs,targets)\n",
        "      tr_loss += loss.item()\n",
        "      big_val,big_idx = torch.max(outputs.data,dim=1)\n",
        "      n_correct += calcuate_accuracy(big_idx,targets)\n",
        "      nb_tr_steps += 1\n",
        "      nb_tr_examples += targets.size(0)\n",
        "\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accuracy = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch:{epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch:{epoch_accuracy}\")\n",
        "\n",
        "    return epoch_accuracy\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SirgRAwcOUo"
      },
      "source": [
        "acc = valid(model, validating_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Db45OlD3LHS"
      },
      "source": [
        "###Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7lum7n9W0xE"
      },
      "source": [
        "#Dataloader for test data\n",
        "class SentimentDatasetTest(Dataset):\n",
        "\n",
        "  def __init__(self,dataframe,tokenizer,max_len):\n",
        "    self.len = len(dataframe)\n",
        "    self.data = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len \n",
        "  \n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    sentence = str(self.data.text[index])\n",
        "    sentence = \" \".join(sentence.split())\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens = True,\n",
        "        max_length = self.max_len,\n",
        "        padding = 'max_length',\n",
        "        return_token_type_ids = False,\n",
        "        return_tensors = 'pt',\n",
        "        truncation = True\n",
        "    )\n",
        "    return {\n",
        "        'ids' : encoding['input_ids'].flatten(),\n",
        "        'mask': encoding['attention_mask'].flatten()\n",
        "    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_oXjZcQXEiT"
      },
      "source": [
        "testing_set = SentimentDatasetTest(test,tokenizer,MAX_LEN)\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V53BQH0GcOtz"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  sentence = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      ids = d[\"ids\"].to(device)\n",
        "      mask = d[\"mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=ids,\n",
        "        attention_mask=mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  return sentence, predictions, prediction_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3x2z768jlSx"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs = get_predictions(\n",
        "  model,\n",
        "  testing_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLTozmtY3ToW"
      },
      "source": [
        "###Write to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mqXhIN3LxbN"
      },
      "source": [
        "y_prediction = y_pred \n",
        "class_name = ['unknown_state','Negative','not-Tamil','Positive','Mixed_feelings']\n",
        "arry = []\n",
        "\n",
        "for y in range(len(y_prediction)): \n",
        "   arry.append(class_name[y_prediction[y].item()])\n",
        "print(arry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irEA-AtzXcn9"
      },
      "source": [
        "a = numpy.array(arry)\n",
        "test_labels_dataframe = pd.DataFrame(a).to_csv(\"/content/drive/My Drive/Sentiment Analysis Fire/output/preprocessed+translation+transliterated/distilmBERT-cased.csv\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}